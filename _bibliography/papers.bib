---
---


@article{teachable,
  abbr={NeurIPS},
  title={Teachable Reinforcement Learning via Advice Distillation},
  author={Watkins, Olivia and Darrel, Trevor and Abbeel, Pieter and Andreas, Jacob and Gupta, Abhishek (final author order tbd)},
  journal={NeurIPS 2021},
  year={2021},
  selected={true},
  code={https://github.com/AliengirlLiv/teachable},
  abstract={We introduce a human-in-the-loop supervision scheme in which an agent first learns to interpret human advice, which can be composed to coach the agent through complex tasks. The agent then distills an advice-independent policy. This lets us train policies on new tasks with less human effort than would be require to train policies through behavioral cloning or reinforcement learning. (Note: paper will be arXived soon.)},
}

@article{du2021auto,
  abbr={ICRA},
  title={Auto-Tuned Sim-to-Real Transfer},
  author={Du *, Yuqing and Watkins*, Olivia and Darrell, Trevor and Abbeel, Pieter and Pathak, Deepak},
  journal={ICRA},
  year={2021},
  selected={true},
  html={https://arxiv.org/abs/2104.07662},
  code={https://yuqingd.github.io/autotuned-sim2real/},
  abstract={We propose a method for automatically tuning robotic simulator system parameters to match the real world using only raw RGB images. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real-world system parameters. We show improvements over domain randomization in simulation and on a real robot.},
}

@article{xrl,
  abbr={ICML Workshop},
  title={Explaining Reinforcement Learning Policies through Counterfactual Trajectories},
  author={Frost, Julius and Watkins, Olivia and Weiner, Eric and Abbeel, Pieter and Darrell, Trevor and Plummer, Bryan and Saenko, Kate},
  journal={ICML Workshop on Human in the Loop Learning },
  year={2021},
  selected={true},
  code={https://github.com/juliusfrost/cfrl},
  abstract={We generate videos of agent behavior to show a human who wishes to understand the agent. We select diverse trajectories by using an exploration to seek out diverse start states. This leads to slight performance improvement on one of two user studies, but in general we find neither this method nor the baselines do much to help users understand agent policies.},
}


@article{drissi2018hierarchical,
  abbr={ICNLP},
  title={Hierarchical text generation using an outline},
  author={Drissi, Mehdi and Watkins, Olivia and Kalita, Jugal},
  selected={true},
  journal={International Conference on Natural Language Processing},
  year={2018},
  html={https://arxiv.org/abs/1810.08802},
  abstract={We propose a method to improve the generation of coherent long-form text by having the model first generate an outline, then generate full text conditioned on the outline. This improves perplexity but does not improve human evaluation.},
}

@article{drissi2018program,
  abbr={ICML Workshop},
  title={Program language translation using a grammar-driven tree-to-tree model},
  author={Drissi*, Mehdi and Watkins*, Olivia and Khant, Aditya and Ojha, Vivaswat and Sandoval, Pedro and Segev, Rakia and Weiner, Eric and Keller, Robert},
  journal={ICML Workshop on Neural Abstract Machines & Program Induction},
  year={2018},
  selected={true},
  html={https://arxiv.org/abs/1807.01784},
  code={https://github.com/hmc-cs-mdrissi/neural_nets_research},
  abstract={We modify existing encoder/decoder approaches for translation between programming language abstract syntax trees by using a grammar-aware decoder which is constrained to only generate syntactically correct programs. This improves performance on a couple synthetic tasks.},
}
